{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## 1.Explain the properties of the F-distribution.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The **F-distribution** is a probability distribution that arises frequently in statistics, especially in analysis of variance (ANOVA), regression analysis, and hypothesis testing. It is used to compare variances and assess whether group means are significantly different. Below are the main properties of the F-distribution:\n\n### 1. **Definition and Purpose**\n- The F-distribution is the ratio of two independent chi-squared distributed variables, each divided by their respective degrees of freedom.\n- It is defined as:\n  \\[\n  F = \\frac{\\frac{S_1^2}{d_1}}{\\frac{S_2^2}{d_2}}\n  \\]\n  where:\n  - \\( S_1^2 \\) and \\( S_2^2 \\) are sample variances.\n  - \\( d_1 \\) and \\( d_2 \\) are the degrees of freedom for the numerator and denominator, respectively.\n\n### 2. **Key Characteristics**\n- **Non-Negative Values**: The F-distribution is always non-negative (\\( F \\geq 0 \\)), as variances cannot be negative.\n- **Asymmetry**: The F-distribution is skewed to the right, with the degree of skewness decreasing as the degrees of freedom increase.\n- **Shape**: The shape depends on the degrees of freedom in the numerator (\\( d_1 \\)) and denominator (\\( d_2 \\)):\n  - For small \\( d_1 \\) and \\( d_2 \\), the distribution is highly skewed.\n  - As \\( d_1 \\) and \\( d_2 \\) increase, the distribution approaches a normal distribution.\n\n### 3. **Degrees of Freedom**\n- The F-distribution is parameterized by two degrees of freedom:\n  - \\( d_1 \\) (numerator degrees of freedom): Related to the number of groups or treatments.\n  - \\( d_2 \\) (denominator degrees of freedom): Related to the number of observations or total sample size.\n- Both \\( d_1 \\) and \\( d_2 \\) must be positive integers.\n\n### 4. **Mean, Variance, and Mode**\n- **Mean**: The mean exists only if \\( d_2 > 2 \\) and is given by:\n  \\[\n  \\text{Mean} = \\frac{d_2}{d_2 - 2}\n  \\]\n- **Variance**: The variance exists only if \\( d_2 > 4 \\) and is given by:\n  \\[\n  \\text{Variance} = \\frac{2 \\cdot d_2^2 \\cdot (d_1 + d_2 - 2)}{d_1 \\cdot (d_2 - 2)^2 \\cdot (d_2 - 4)}\n  \\]\n- **Mode**: The mode is defined for \\( d_1 > 2 \\) and is given by:\n  \\[\n  \\text{Mode} = \\frac{(d_1 - 2)}{d_1} \\cdot \\frac{d_2}{d_2 + 2}\n  \\]\n\n### 5. **Applications**\n- **ANOVA**: The F-distribution is used to determine if there are significant differences between group means by comparing the variance between groups to the variance within groups.\n- **Regression Analysis**: It tests the overall significance of the regression model by comparing the variance explained by the model to the unexplained variance.\n- **Hypothesis Testing**: Tests for equality of variances, e.g., in Levene’s test or Bartlett’s test.\n\n### 6. **Cumulative Distribution Function (CDF)**\n- The CDF of the F-distribution represents the probability that a random variable \\( F \\) is less than or equal to a given value.\n- Tables and computational tools (e.g., statistical software) are often used to calculate probabilities associated with the F-distribution, as its CDF does not have a simple closed-form expression.\n\n### 7. **Relationships with Other Distributions**\n- The F-distribution is related to the chi-squared distribution. Specifically, if \\( X_1 \\sim \\chi^2(d_1) \\) and \\( X_2 \\sim \\chi^2(d_2) \\), and \\( X_1 \\) and \\( X_2 \\) are independent, then:\n  \\[\n  F = \\frac{\\left(X_1 / d_1\\right)}{\\left(X_2 / d_2\\right)} \\sim F(d_1, d_2)\n  \\]\n\n### Summary\nThe F-distribution is a versatile statistical tool used primarily for variance comparisons. Its properties, such as its dependency on degrees of freedom and its right-skewed nature, make it especially useful in hypothesis testing for complex models.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The **F-distribution** is widely used in several types of statistical tests, primarily when comparing variances or testing multiple group differences. It is appropriate for these tests because it arises naturally when ratios of variances are considered, especially under the assumption that the data follows a normal distribution. Below are the main types of tests where the F-distribution is used and why it is suitable:\n\n### 1. **Analysis of Variance (ANOVA)**\n- **Purpose**: ANOVA tests whether the means of multiple groups are significantly different.\n- **Why F-distribution is appropriate**:\n  - ANOVA calculates the ratio of *between-group variance* to *within-group variance*.\n  - The F-distribution models the expected variation in this ratio under the null hypothesis (no difference in group means).\n  - The test statistic follows an F-distribution because both variances in the numerator and denominator are independent and chi-squared distributed.\n\n### 2. **Regression Analysis**\n- **Purpose**: The F-test in regression evaluates the overall significance of a regression model, i.e., whether at least one predictor variable is related to the dependent variable.\n- **Why F-distribution is appropriate**:\n  - The test compares the variance explained by the regression model (due to predictors) with the residual variance (unexplained by the model).\n  - The ratio of these variances forms the F-statistic, which follows an F-distribution under the null hypothesis that all regression coefficients (except the intercept) are zero.\n\n### 3. **Tests for Equality of Variances**\n- **Purpose**: These tests check whether two or more population variances are equal. Examples include:\n  - **Levene's Test**: Robust to deviations from normality.\n  - **Bartlett's Test**: More sensitive under normality assumptions.\n- **Why F-distribution is appropriate**:\n  - Variances are proportional to chi-squared distributions, and the ratio of two chi-squared variables forms an F-distribution.\n  - This allows for testing whether the observed variance ratio is significantly different from 1 under the null hypothesis.\n\n### 4. **General Linear Models**\n- **Purpose**: In models like multivariate regression, MANOVA, or ANCOVA, the F-test evaluates the significance of model terms (e.g., factors or interactions).\n- **Why F-distribution is appropriate**:\n  - These tests involve comparing the explained variance for a specific model term with the residual variance, analogous to ANOVA.\n  - The F-statistic naturally follows the F-distribution because it is a ratio of independent variances.\n\n### 5. **Model Comparisons (Nested Models)**\n- **Purpose**: The F-test is used to compare two nested models, where one model is a special case of the other (fewer parameters).\n- **Why F-distribution is appropriate**:\n  - The test evaluates whether adding parameters significantly improves the fit of the model.\n  - The ratio of the model improvement to the residual variance follows an F-distribution.\n\n### 6. **Two-Way ANOVA and Beyond**\n- **Purpose**: Extends ANOVA to test multiple factors and their interactions (e.g., Two-Way ANOVA or Factorial ANOVA).\n- **Why F-distribution is appropriate**:\n  - Similar to one-way ANOVA, the test relies on partitioning the total variance and comparing these variance components using an F-ratio.\n\n### Summary of Why the F-Distribution is Appropriate\nThe F-distribution is appropriate for these tests because:\n1. It models the ratio of independent variances, which is central to the design of these tests.\n2. It accounts for the variability in estimates due to sample size via degrees of freedom.\n3. It has well-defined critical values for hypothesis testing, allowing for precise decision-making under the null hypothesis. \n\nBy leveraging its properties, the F-distribution provides a rigorous framework for comparing variances and testing hypotheses in a wide range of statistical contexts.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Conducting an F-test to compare the variances of two populations involves specific assumptions that must be met to ensure the validity of the test results. The key assumptions are:\n\n### 1. **Independence of Samples**\n- The two samples must be **independent** of each other. This means the data in one sample should not influence or be related to the data in the other sample.\n\n### 2. **Normality of Populations**\n- Both populations from which the samples are drawn must follow a **normal distribution**. \n- The F-test is sensitive to deviations from normality, and violations of this assumption can lead to inaccurate results (e.g., inflated Type I error rates).\n\n### 3. **Random Sampling**\n- The samples must be obtained through a **random sampling process** to ensure that they are representative of their respective populations.\n\n### 4. **Measurement Scale**\n- The data should be measured on a **continuous scale** (interval or ratio) since the test involves calculating variances, which require numerical values.\n\n### 5. **Positive Variances**\n- The variances being compared must be **positive**, as negative variances are not meaningful in the context of an F-test.\n\n### 6. **Equality of Variances (Under Null Hypothesis)**\n- The null hypothesis of the F-test assumes that the population variances are equal:\n  \\[\n  H_0: \\sigma_1^2 = \\sigma_2^2\n  \\]\n  The alternative hypothesis typically states that the variances are not equal:\n  \\[\n  H_a: \\sigma_1^2 \\neq \\sigma_2^2\n  \\]\n  (or one-sided alternatives, depending on the context).\n\n### Implications of Violating Assumptions:\n1. **Non-Normality**: If populations are not normally distributed, the F-test can give misleading results. In such cases, alternative tests like Levene’s test or Bartlett’s test, or non-parametric methods, might be more appropriate.\n2. **Dependent Samples**: If the samples are not independent, paired tests or adjustments to the analysis are required.\n3. **Robustness**: While the F-test is sensitive to normality, it can be relatively robust if sample sizes are large and approximately equal.\n\nBy ensuring these assumptions are met, the F-test can reliably determine whether there is a significant difference between the variances of two populations.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 4. What is the purpose of ANOVA, and how does it differ from a t-test? ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Purpose of ANOVA:**\nThe primary purpose of **Analysis of Variance (ANOVA)** is to determine whether there are statistically significant differences among the means of three or more groups. It does this by analyzing the variance within groups compared to the variance between groups. \n\n- **Null Hypothesis (\\(H_0\\))**: All group means are equal (\\( \\mu_1 = \\mu_2 = \\dots = \\mu_k \\)).\n- **Alternative Hypothesis (\\(H_a\\))**: At least one group mean is different.\n\nANOVA is particularly useful in experimental and observational studies to evaluate whether a factor (independent variable) has an effect on a dependent variable.\n\n**How ANOVA Differs from a t-test:**\n\n| **Aspect**                | **t-test**                                    | **ANOVA**                                 |\n|---------------------------|-----------------------------------------------|------------------------------------------|\n| **Number of Groups**      | Compares **two groups** only.                 | Compares **three or more groups**.       |\n| **Hypotheses**            | Tests the difference between two means (\\( \\mu_1 \\neq \\mu_2 \\)). | Tests if there is a difference among multiple means (\\( \\mu_1, \\mu_2, \\dots, \\mu_k \\)). |\n| **Variance Comparison**   | Typically assumes equal variances (independent samples t-test) but doesn't directly compare variances. | Explicitly partitions variance into *between-group* and *within-group* components. |\n| **Type of Test Statistic**| Uses a **t-statistic**.                       | Uses an **F-statistic**.                 |\n| **Application Scope**     | Limited to pairwise comparisons.              | Suitable for comparing multiple groups simultaneously. |\n| **Risk of Type I Error**  | Repeated t-tests increase the risk of Type I error when comparing multiple groups. | Controls the overall Type I error when comparing multiple groups. |\n| **Post-hoc Analysis**     | Not required since only two groups are compared. | Requires post-hoc tests (e.g., Tukey, Bonferroni) to identify which groups differ if the null is rejected. |\n\n### When to Use ANOVA vs. a t-test:\n1. **Use a t-test** if:\n   - You are comparing the means of **exactly two groups**.\n   - You are interested in whether there is a significant difference between these two specific groups.\n\n2. **Use ANOVA** if:\n   - You are comparing the means of **three or more groups**.\n   - You want to determine if there is a general effect of a factor, without specifying which groups differ initially.\n\n### Example:\n- **t-test Scenario**: Testing whether the average height of men (\\( \\mu_1 \\)) differs from women (\\( \\mu_2 \\)).\n- **ANOVA Scenario**: Testing whether the average height differs among three regions (e.g., \\( \\mu_1 \\): North, \\( \\mu_2 \\): South, \\( \\mu_3 \\): East).\n\nANOVA is a more general tool for comparing multiple groups, reducing the likelihood of false positives compared to running multiple t-tests.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "You would use a **one-way ANOVA** instead of multiple t-tests when comparing the means of more than two groups for several important reasons:\n\n### **When to Use a One-Way ANOVA**\n1. **Number of Groups**:\n   - Use a one-way ANOVA when comparing **three or more groups** on a single factor (independent variable).\n   - Example: Comparing the average test scores of students from three different schools (School A, School B, School C).\n\n2. **Purpose**:\n   - To determine if there is a statistically significant difference **among group means**, without initially specifying which groups differ.\n\n3. **Experimental Design**:\n   - The study involves a **single independent variable** with multiple levels (e.g., treatments, conditions, or groups).\n   - Example: Testing the effect of three types of diets (low-carb, low-fat, and high-protein) on weight loss.\n\n### **Why Use One-Way ANOVA Instead of Multiple t-Tests**\n\n#### 1. **Avoiding Inflation of Type I Error Rate**\n- Conducting multiple t-tests increases the probability of making a **Type I error** (incorrectly rejecting the null hypothesis).\n  - If the significance level for each t-test is \\( \\alpha = 0.05 \\), the overall error rate increases as more comparisons are made:\n    \\[\n    \\text{Overall Type I Error Rate} = 1 - (1 - \\alpha)^k\n    \\]\n    where \\( k \\) is the number of pairwise comparisons.\n  - For example, comparing 3 groups with 3 t-tests inflates the error rate to about 14%, not 5%.\n\n- **ANOVA controls the overall Type I error rate** by testing all group differences simultaneously at the same significance level.\n\n#### 2. **Efficiency**\n- ANOVA compares all groups in **a single test**, avoiding the need to conduct multiple pairwise comparisons.\n- This reduces the computational burden and simplifies the interpretation of results.\n\n#### 3. **Holistic Analysis**\n- ANOVA tests the **overall null hypothesis**:\n  \\[\n  H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k\n  \\]\n  It determines whether there is a significant difference **among any of the group means**, without requiring separate tests for each pair of groups.\n\n#### 4. **Post-hoc Testing for Specific Differences**\n- If the ANOVA result is significant, post-hoc tests (e.g., Tukey's HSD, Bonferroni correction) can be used to identify which specific groups differ, while still controlling for multiple comparisons.\n\n### **Example**\n\n#### Scenario:\nYou want to compare the average response times of participants under three different conditions:\n- Condition A (no distractions)\n- Condition B (low distractions)\n- Condition C (high distractions).\n\n#### Why ANOVA:\n- Running t-tests for each pair (A vs. B, A vs. C, B vs. C) increases the risk of false positives.\n- One-way ANOVA evaluates whether there are any differences across the three groups in a single test, ensuring a controlled error rate.\n\n### Summary\nA one-way ANOVA is preferred over multiple t-tests for comparing more than two groups because it:\n- **Controls Type I error** when testing multiple groups simultaneously.\n- **Streamlines analysis** by performing a single overall test.\n- Allows for **post-hoc comparisons** if significant differences are found. \n\nThis makes ANOVA a more robust and statistically sound method for multi-group comparisons.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 6.  Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In **Analysis of Variance (ANOVA)**, variance is partitioned into two key components: **between-group variance** and **within-group variance**. This partitioning is central to the calculation of the **F-statistic**, which determines whether there are statistically significant differences among group means. Here's a detailed explanation:\n\n### **Variance Partitioning in ANOVA**\n\n1. **Total Variance (Total Sum of Squares, SST)**:\n   - Represents the overall variability in the data, measured as the sum of squared deviations of each observation from the overall mean (\\( \\bar{Y} \\)).\n   \\[\n   SST = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y})^2\n   \\]\n   where:\n   - \\( Y_{ij} \\) is the \\( j \\)-th observation in group \\( i \\).\n   - \\( \\bar{Y} \\) is the overall mean.\n   - \\( k \\) is the number of groups.\n   - \\( n_i \\) is the number of observations in group \\( i \\).\n\n   **Partitioning of SST**:\n   \\[\n   SST = SSB + SSW\n   \\]\n   where:\n   - \\( SSB \\): Sum of Squares Between Groups.\n   - \\( SSW \\): Sum of Squares Within Groups.\n\n2. **Between-Group Variance (Sum of Squares Between Groups, SSB)**:\n   - Measures variability **between the group means** and the overall mean.\n   - It quantifies how much the group means differ from one another relative to the overall mean.\n   \\[\n   SSB = \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y})^2\n   \\]\n   where:\n   - \\( \\bar{Y}_i \\) is the mean of group \\( i \\).\n   - \\( n_i \\) is the number of observations in group \\( i \\).\n   - \\( \\bar{Y} \\) is the overall mean.\n\n   - **Large \\( SSB \\)** indicates that group means are far apart, suggesting a potential effect of the grouping factor.\n\n3. **Within-Group Variance (Sum of Squares Within Groups, SSW)**:\n   - Measures variability **within each group**, i.e., how much individual observations deviate from their group mean.\n   - It represents the natural variability within the data unrelated to the grouping factor.\n   \\[\n   SSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n   \\]\n\n   - **Small \\( SSW \\)** indicates that observations within each group are close to their respective group means.\n     \n### **Contribution to the F-Statistic**\n\n1. **Mean Squares (MS):**\n   - To standardize the sums of squares, they are divided by their respective degrees of freedom to calculate the **mean squares**:\n     - \\( MSB \\) (Mean Square Between Groups):\n       \\[\n       MSB = \\frac{SSB}{k - 1}\n       \\]\n       where \\( k - 1 \\) is the degrees of freedom for between-group variability.\n     - \\( MSW \\) (Mean Square Within Groups):\n       \\[\n       MSW = \\frac{SSW}{N - k}\n       \\]\n       where \\( N - k \\) is the degrees of freedom for within-group variability (\\( N \\) is the total number of observations).\n\n2. **F-Statistic**:\n   - The **F-statistic** is the ratio of the between-group mean square to the within-group mean square:\n     \\[\n     F = \\frac{MSB}{MSW}\n     \\]\n   - **Interpretation**:\n     - A large \\( F \\)-value indicates that \\( MSB \\) is much larger than \\( MSW \\), suggesting that the differences between group means are greater than what could be expected due to random variation (within-group variance).\n     - Under the null hypothesis (\\( H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k \\)), \\( MSB \\) and \\( MSW \\) estimate the same variance, and \\( F \\) is expected to follow an F-distribution.\n\n### **Summary of Contributions to the F-Statistic**\n- **Between-group variance (MSB)** captures variability due to differences between group means (signal).\n- **Within-group variance (MSW)** captures variability due to random noise or natural variation within groups (noise).\n- The F-statistic compares the ratio of signal (MSB) to noise (MSW). A significantly large \\( F \\)-statistic indicates that group differences are unlikely to be due to chance, leading to rejection of the null hypothesis.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The **classical (frequentist)** approach to ANOVA and the **Bayesian** approach to ANOVA both aim to determine whether there are significant differences among group means, but they differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing. Below is a comparison of the two approaches:\n\n### **1. Treatment of Uncertainty**\n#### Frequentist Approach:\n- Uncertainty is addressed using **probabilities based on repeated sampling**.\n- Variability in data is attributed to random sampling error, and conclusions are based on the **long-run behavior** of the test statistic under the null hypothesis.\n- Uncertainty about parameters is not directly modeled; instead, it is inferred from sample data via confidence intervals and p-values.\n\n#### Bayesian Approach:\n- Uncertainty is modeled explicitly through **probability distributions**.\n- Parameters (e.g., group means, variances) are treated as **random variables** with prior distributions reflecting prior knowledge or beliefs.\n- The posterior distribution, derived using Bayes' theorem, combines prior information and observed data to quantify uncertainty about parameters.\n\n### **2. Parameter Estimation**\n#### Frequentist Approach:\n- Parameters (e.g., group means and variances) are estimated using **point estimates** (e.g., sample means, pooled variances).\n- The analysis does not provide a distribution for the parameter itself but rather uses sampling distributions to construct confidence intervals.\n- Estimation is based solely on the observed data.\n\n#### Bayesian Approach:\n- Parameters are estimated using the **posterior distribution**, which represents the updated belief about the parameter after observing the data.\n- Estimation typically involves summarizing the posterior with metrics like the **posterior mean**, **median**, or **credible intervals** (analogous to confidence intervals).\n- The prior distribution allows incorporation of external knowledge or beliefs into the analysis.\n\n### **3. Hypothesis Testing**\n#### Frequentist Approach:\n- Hypothesis testing involves assessing a **null hypothesis** (e.g., \\( H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k \\)) using the F-statistic.\n- A **p-value** is calculated to determine the probability of observing the data (or something more extreme) under the null hypothesis.\n- Decisions are binary: reject or fail to reject \\( H_0 \\), based on a predefined significance level (\\( \\alpha \\)).\n\n#### Bayesian Approach:\n- Bayesian analysis often avoids a strict null hypothesis in favor of evaluating the **probability of competing models** or estimating parameters directly.\n- Model comparison is conducted using metrics like the **Bayes Factor** (the ratio of posterior probabilities for two models) or posterior probabilities.\n- Decisions are probabilistic and interpretive, allowing for statements like \"There is an 80% probability that the means differ significantly.\"\n\n### **4. Prior Information**\n#### Frequentist Approach:\n- Does not incorporate prior information about parameters; the analysis is based entirely on the observed data.\n- Assumes that the data alone provide sufficient information for inference.\n\n#### Bayesian Approach:\n- Requires specification of **prior distributions** for parameters (e.g., group means, variances).\n- Priors can be **informative** (if prior knowledge exists) or **non-informative** (if no prior knowledge is available).\n- The choice of prior can significantly affect results, especially with limited data.\n\n### **5. Interpretation of Results**\n#### Frequentist Approach:\n- Results are interpreted in terms of **long-run probabilities**:\n  - E.g., \"If the null hypothesis were true, we would observe this F-statistic (or a more extreme one) with a probability of 0.03.\"\n- Confidence intervals are interpreted as ranges that, in repeated sampling, would capture the true parameter \\( 1 - \\alpha \\) of the time.\n\n#### Bayesian Approach:\n- Results are interpreted in terms of **probabilities about parameters**:\n  - E.g., \"The posterior probability that the group mean lies between 3.5 and 4.5 is 95%.\"\n- Credible intervals provide a direct probability statement about the parameter.\n\n### **6. Computational Complexity**\n#### Frequentist Approach:\n- Typically computationally simpler, relying on analytical solutions and straightforward calculations of sums of squares and F-statistics.\n\n#### Bayesian Approach:\n- Often computationally intensive, especially for complex models.\n- Requires numerical methods like **Markov Chain Monte Carlo (MCMC)** to estimate posterior distributions.\n\n### **Summary of Key Differences**\n\n| **Aspect**                  | **Frequentist ANOVA**                      | **Bayesian ANOVA**                           |\n|-----------------------------|-------------------------------------------|---------------------------------------------|\n| **Uncertainty**             | Based on sampling variability             | Modeled via probability distributions        |\n| **Parameter Estimation**    | Point estimates and confidence intervals  | Posterior distributions and credible intervals |\n| **Hypothesis Testing**      | Null hypothesis, p-values                 | Posterior probabilities, Bayes factors       |\n| **Use of Prior Knowledge**  | None                                      | Requires priors (can be informative or flat) |\n| **Interpretation**          | Long-run frequency interpretation         | Probability statements about parameters      |\n| **Computation**             | Relatively simple                        | Often computationally intensive              |\n\n### **When to Use Each Approach**\n- **Frequentist ANOVA**: Suitable for straightforward, hypothesis-driven analyses with sufficient data and no need to incorporate prior knowledge.\n- **Bayesian ANOVA**: Preferred when:\n  - Prior information is available or needed.\n  - A more nuanced, probabilistic interpretation of results is desired.\n  - Data are sparse or noisy, making it useful to incorporate prior distributions for stability.\n\nEach approach has its strengths, and the choice depends on the research context, the nature of the data, and the goals of the analysis.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 8. You have two sets of data representing the incomes of two different professions: \nProfession A2 [48, 52, 55, 60, 62]\n\nProfession B2 [45, 50, 55, 52, 47] \n\nPerform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom scipy.stats import f\n\n# Data for the two professions\nprofession_A2 = np.array([48, 52, 55, 60, 62])\nprofession_B2 = np.array([45, 50, 55, 52, 47])\n\n# Calculate the variances of the two samples\nvar_A2 = np.var(profession_A2, ddof=1)  # Sample variance for Profession A2\nvar_B2 = np.var(profession_B2, ddof=1)  # Sample variance for Profession B2\n\n# Calculate the F-statistic\nF_statistic = var_A2 / var_B2\n\n# Degrees of freedom\ndf1 = len(profession_A2) - 1\ndf2 = len(profession_B2) - 1\n\n# Calculate the p-value (two-tailed test)\np_value = 2 * min(f.cdf(F_statistic, df1, df2), 1 - f.cdf(F_statistic, df1, df2))\n\n# Results\nF_statistic, p_value\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(2.089171974522293, 0.49304859900533904)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "The results of the F-test are as follows:\n\n- **F-statistic**: 2.0892  \n- **p-value**: 0.4930  \n\n### Conclusion:\n1. The **null hypothesis** of the F-test states that the variances of the two populations (incomes of Profession A2 and B2) are equal.\n2. Since the p-value (0.4930) is significantly greater than a typical significance level (e.g., \\( \\alpha = 0.05 \\)), we **fail to reject the null hypothesis**.\n3. This means there is no statistically significant evidence to conclude that the variances of the two professions' incomes are different. \n\nThe incomes of the two professions appear to have comparable variability.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 9. Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:\nRegion A2 [160, 162, 165, 158, 164]\n\nRegion B2 [172, 175, 170, 168, 174]\n\nRegion C2 [180, 182, 179, 185, 183]",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from scipy.stats import f_oneway\n\n# Data for the three regions\nregion_A2 = [160, 162, 165, 158, 164]\nregion_B2 = [172, 175, 170, 168, 174]\nregion_C2 = [180, 182, 179, 185, 183]\n\n# Perform one-way ANOVA\nf_statistic, p_value = f_oneway(region_A2, region_B2, region_C2)\n\n# Results\nf_statistic, p_value\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(67.87330316742101, 2.870664187937026e-07)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "The results of the one-way ANOVA are as follows:\n\n- **F-statistic**: 67.87  \n- **p-value**: \\( 2.87 \\times 10^{-7} \\) (very close to zero)\n\n### Interpretation:\n1. The **null hypothesis** for one-way ANOVA states that the means of the three regions are equal (\\( \\mu_{\\text{A2}} = \\mu_{\\text{B2}} = \\mu_{\\text{C2}} \\)).\n2. Since the p-value is much smaller than a typical significance level (e.g., \\( \\alpha = 0.05 \\)), we **reject the null hypothesis**.\n3. This means there is strong evidence to conclude that there are statistically significant differences in average heights among the three regions.\n\nTo determine which specific regions differ, post-hoc tests (e.g., Tukey's HSD) would be needed.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}